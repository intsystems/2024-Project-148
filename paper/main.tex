\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[russian, english]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}



\title{Weighted cocherence as topic models' interpretability measure}

\author{ 
\textbf{Zhgutov K. D.} (\texttt{zhgutov.kd@phystech.edu}) \\
\textbf{Alekseev V. A.} (\texttt{wasya.alekseev@gmail.com}) \\
\textbf{Vorontsov K. V.} (\texttt{vokov@forecsys.ru}) \\
\\
Moscow Institute of Physics and Technology
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{Weighted cocherence as topic models' interpretability measure}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Weighted cocherence as topic models' interpretability measure},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={topic modeling, topic coherence, topic interpretability, topic model, BigARTM, text analysis, machine learning},
}

\begin{document}
\maketitle

\begin{abstract}
Topic modeling is very useful for analyzing text data. It can be used to analyze large collection of text data such as articles, reviews, social media, and others. This helps in clusterization documents by topic, extracting keywords, and identifying patterns in the data. There are a lot of automatically calculated criteria of informativeness of thematic models. One of these criteria is coherence. But the problem with coherence is that it does not take into account most of the text in the calculation, which makes evaluating the quality of the topic by this critera unreliable. The aim is to propose a new method for calculating coherence that takes into account the distribution of the topic throughout the text.
\end{abstract}


\keywords{topic modeling \and topic coherence \and topic interpretability \and topic model \and BigARTM \and text analysis \and machine learning}

\section{Introduction}
Topic modeling is a text data analysis method that automatically identifies hidden topics in large collections of text data. Topic models are used in information retrieval, documents’ categorization, social networks’ data analysis, recommendation systems, exploratory search and other areas.

Interpretability is a key characteristic of an effective topic model. But interpretability of the topic model is a poorly formalized requirement. Informally, it means that according to the lists of the most frequent words of the topic, the expert can understand what this topic is about and give it an adequate name. Expert approaches are necessary at the research stage, but they make it difficult to automatically build good topic model.

It was previously shown [2] that among the quality criteria calculated automatically from a collection, consistency or coherence correlates best with expert estimates of interpretability. However, the previously proposed methods of calculating coherence have a fundamental limitation. They take into account the distribution of only a very small number of words, which leads to a significant loss of accuracy.

This study aims to advance coherence calculation techniques by exploring new quality criteria for topic models that take into account the distribution of topics throughout the text. The research compares these new criteria with existing methods and proposes a methodology for assessing the interpretability of topics.

\section{Problem statement}

Let $D$ denote a set (collection) of texts and $W$ denote a set (vocabulary) of all terms from these texts. Each term can represent a single word as well as a key phrase. Each document $d \in D$ is a sequence of $n_d$ terms $(w_1, \dots, w_n)$ from the vocabulary $W$. Each term might appear multiple times in the same document.

Assume that each term occurrence in each document refers to some latent topic from a finite set of topics T. Text collection is considered to be a sample of triples $(w_i,d_i,t_i),\, {i=1,\ldots ,n} $ drawn independently from a discrete distribution $p(w,d,t)$ over a finite space $W\times D \times T$. Term w and document $d$ are observable variables, while topic t is a latent (hidden) variable. Following the “bag of words” model, we represent each document by a subset of terms $d\subset W$ and the corresponding integers $n_{dw}$, which count how many times the term $w$ appears in the document $d$.

Conditional independence is an assumption that each topic generates terms regardless of the document: $p(w\ {\vert }\ t) = p(w\ {\vert }\ d,t)$. According to the law of total probability and the assumption of conditional independence

$\begin{aligned} p(w\ {\vert }\ d) = \sum _{t\in T} p(t\ {\vert }\ d)\, p(w\ {\vert }\ t)\end{aligned}$ (1)

The probabilistic model (1) describes how the collection D is generated from the known distributions $p(t\ {\vert }\ d)$ and $p(w\ {\vert }\ t)$. Learning a topic model is an inverse problem: to find distributions $p(t\ {\vert }\ d)$ and $p(w\ {\vert }\ t)$ given a collection $D$. This problem is equivalent to finding an approximate representation of counter matrix

$\begin{aligned} F = \bigl ( \hat{p}_{wd} \bigr )_{W{\times }D}, \quad \hat{p}_{wd} = \hat{p}(w\ {\vert }\ d) = \tfrac{n_{dw}}{n_d}  \end{aligned}$ (2)

as a product $F \approx \Phi \Theta $ of two unknown matrices—the matrix $\Phi$ of term probabilities for the topics and the matrix $\Theta$ of topic probabilities for the documents:

$\begin{aligned} \begin{array}{rlrlrl} \Phi &{}= (\phi _{wt})_{W{\times }T},\;\;\;\; &{} \phi _{wt} &{}= p(w\ {\vert }\ t),\;\;\;\; &{} \phi _t &{}= (\phi _{wt})_{w\in W} \\ \Theta &{}= (\theta _{td})_{T{\times }D},\;\;\;\; &{} \theta _{td} &{}= p(t\ {\vert }\ d),\;\;\;\; &{} \theta _d &{}= (\theta _{td})_{t\in T}\end{array} \end{aligned}$

Lets define weithed coherency

$coh_t = \frac {\sum_{u, v} \text{rel}_t(u, v) \text{coh}(u, v)} {\sum_{u, v}  \text{rel}_t(u, v)}$

Our goal is to find functions $\text{rel}_t(u, v), \text{coh}(u, v)$ that shows best correlation with human estimations of topic interpretability.  




\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}
